{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Installing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install requests faiss-cpu sentence-transformers beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import faiss\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import json\n",
    "\n",
    "# Initialize embedding model (Sentence-BERT for efficiency)\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "BASE_URL = \"https://devgan.in\"\n",
    "LAW_TYPES = [\"ipc\", \"bns\"]  # Both IPC and BNS will be stored together\n",
    "\n",
    "# FAISS setup\n",
    "d = 384  # Dimension of embeddings (for MiniLM, it's 384)\n",
    "index = faiss.IndexFlatL2(d)\n",
    "documents = []  # To store metadata\n",
    "\n",
    "for LAW_TYPE in LAW_TYPES:\n",
    "    MAIN_URL = f\"{BASE_URL}/{LAW_TYPE}/\"\n",
    "    response = requests.get(MAIN_URL)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    chapters = []\n",
    "    for row in soup.select(\"table.menu tr\"):\n",
    "        columns = row.find_all(\"td\")\n",
    "        if len(columns) == 2:\n",
    "            chapter_number = columns[0].text.strip()\n",
    "            chapter_title = columns[1].text.strip()\n",
    "            chapter_link = BASE_URL + columns[1].find(\"a\")[\"href\"]\n",
    "            chapters.append((chapter_number, chapter_title, chapter_link))\n",
    "\n",
    "    # Scrape each chapter's content\n",
    "    for chapter_number, chapter_title, chapter_link in chapters:\n",
    "        chapter_response = requests.get(chapter_link)\n",
    "        chapter_soup = BeautifulSoup(chapter_response.text, \"html.parser\")\n",
    "        content_div = chapter_soup.find(\"div\", id=\"content\")\n",
    "\n",
    "        if content_div:\n",
    "            chapter_content = content_div.get_text(separator=\"\\n\", strip=True)\n",
    "        else:\n",
    "            chapter_content = \"Content not found.\"\n",
    "\n",
    "        # Generate embeddings and store in FAISS\n",
    "        embedding = model.encode(chapter_content).astype(np.float32)\n",
    "        index.add(np.array([embedding]))\n",
    "\n",
    "        # Store metadata\n",
    "        documents.append({\n",
    "            \"law_type\": LAW_TYPE.upper(),\n",
    "            \"chapter_number\": chapter_number,\n",
    "            \"chapter_title\": chapter_title,\n",
    "            \"content\": chapter_content,\n",
    "            \"source_url\": chapter_link\n",
    "        })\n",
    "\n",
    "index = faiss.write_index(index, \"law_faiss.index\")\n",
    "\n",
    "# Save metadata\n",
    "with open(\"law_metadata.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(documents, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"Scraping and FAISS indexing complete!\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
